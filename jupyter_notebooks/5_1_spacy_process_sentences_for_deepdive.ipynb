{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add bags of words from additional arousal and non-arousal datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Start postgres if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good spacy intro for getting tokens\n",
    "https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "\n",
    "## Load spacy.io\n",
    "https://spacy.io/#install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import codecs\n",
    "from spacy.en import English\n",
    "nlp = English(parser=True, tagger=True) # so we can sentence parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather positive and negative sentences for Arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the annotations for Arousal topic as annotated by Mark.\n",
    "\n",
    "    See 2_spacy_parse_annotations notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ccarey/Documents/Projects/NAMI/rdoc/results\n"
     ]
    }
   ],
   "source": [
    "wkdir = '/Users/ccarey/Documents/Projects/NAMI/rdoc/results'\n",
    "%cd $wkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     201    5280   38255 ./annotations_processed/AR_mk_pos\n",
      "     166    4054   29188 ./annotations_processed/AR_mk_neg\n",
      "     211    4215   29329 ./annotations_processed/AR_mk_not_pos\n",
      "annotations_processed/AR_mk_pos: text/plain; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "!wc ./annotations_processed/AR_mk_pos\n",
    "!wc ./annotations_processed/AR_mk_neg\n",
    "!wc ./annotations_processed/AR_mk_not_pos\n",
    "! file -I annotations_processed/AR_mk_pos # utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sentences = []\n",
    "with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "    sents = f.read().splitlines() # 201\n",
    "    for s in sents:\n",
    "        s = s.replace('{{', '').replace('}}', '').strip()\n",
    "        pos_sentences.append(s)\n",
    "len(pos_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_sentences = []\n",
    "with codecs.open('./annotations_processed/AR_mk_not_pos', mode='r', encoding='utf-8') as f:\n",
    "    sents = f.read().splitlines() # 201\n",
    "    for s in sents:\n",
    "        s = s.replace('{{', '').replace('}}', '').strip()\n",
    "        neutral_sentences.append(s)\n",
    "len(neutral_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a temporary set of negative sentences for Arousal by Mark.\n",
    "\n",
    "They are annotated in abstracts that were not in the arousal topic\n",
    "    See 2_spacy_parse_annotations notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sentences = []\n",
    "with codecs.open('./annotations_processed/AR_mk_neg', mode='r', encoding='utf-8') as f:\n",
    "    neg_sents = f.read().splitlines() # 201\n",
    "    for s in neg_sents:\n",
    "        s = s.replace('{{', '').replace('}}', '').strip()\n",
    "        neg_sentences.append(s)\n",
    "len(neg_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spacy to create bags of words from each sentence.\n",
    "#TODO: not sure if we should unique the bag of words. If encoding to integer feature vector, we would be counting occurrences.\n",
    "### spacy helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_split_sentences(text):\n",
    "    sentences = []\n",
    "    #doc = nlp(text.decode('utf8')) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    doc = nlp(text) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    for span in doc.sents:\n",
    "        #sentences.append(u''.join(doc[i].string for i in range(span.start, span.end)).encode('utf-8').strip())\n",
    "        sentences.append(''.join(doc[i].string for i in range(span.start, span.end)))#.strip())\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_lemma_gt_len(text, length=2):\n",
    "    '''Create bag of unique lemmas, requiring lemma length > length\n",
    "    \n",
    "    Note, not counting possible multiple occurrences of words / lemmas.\n",
    "    \n",
    "    (That caution in order as Some approaches might prefer to score by word frequency as well as by the word.)\n",
    "\n",
    "    Note: setting length to 1 may mess up our postgres arrays as we would\n",
    "    get commas here, unless we were to quote everything.\n",
    "    '''\n",
    "    tokens = []\n",
    "    #doc = nlp(text.decode('utf8')) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    parsed_data = nlp(text) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    for token in parsed_data:\n",
    "        if len(token.lemma_) > length:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    return(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'rotten', u'there', u'inside', u'apple']\n",
      "there inside, rotten worm, tell me, once tell, worm there, inside ., apple once, me there, there be, a rotten\n"
     ]
    }
   ],
   "source": [
    "def spacy_lemma_biwords_gt_len(text, length=3):\n",
    "    '''Create bag of unique bi-lemmas, requiring lemma length > length\n",
    "    \n",
    "    Note, not counting possible multiple occurrences of words / lemmas.\n",
    "    \n",
    "    (That caution in order as Some approaches might prefer to score by word frequency as well as by the word.)\n",
    "    \n",
    "    We are crudely eliminating any bi-lemmas that have commas in them to save us in loading postgres arrays.\n",
    "    '''\n",
    "    biwords = []\n",
    "    parsed_data = nlp(text)\n",
    "    skip_chars = [',', '\"', \"'\"]\n",
    "    for i in range(1, len(parsed_data) - 1):\n",
    "        skip = False\n",
    "        biword = u'{} {}'.format(parsed_data[i].lemma_.lower(), parsed_data[i+1].lemma_.lower())\n",
    "        if (parsed_data[i].lemma_ in skip_chars or parsed_data[i+1].lemma_ in skip_chars):\n",
    "            skip = True\n",
    "        if len(biword) > length and not skip:\n",
    "            biwords.append(biword)\n",
    "    return(list(set(biwords)))\n",
    "\n",
    "test = 'A good, apple once told me there was a rotten worm there inside.'.decode('utf8')\n",
    "res = spacy_lemma_gt_len(test, length=4)\n",
    "print(res)\n",
    "res = spacy_lemma_biwords_gt_len(test, length=4)\n",
    "# ', '.join(res) # note, flattens formatting.\n",
    "print(', '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_lemmas_and_lemma_biwords(sentence, lemma_len=2, bi_lemma_len=4):\n",
    "    lemma = spacy_lemma_gt_len(sentence, length=lemma_len)\n",
    "    lemma_biwords = spacy_lemma_biwords_gt_len(sentence, length=bi_lemma_len)\n",
    "    return(lemma + lemma_biwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidenote: what do we use as sentences?\n",
    "\n",
    "We preserve sentences as parsed in stop 2_spacy_parse_annotations notebook.\n",
    "\n",
    "Note, we did some manual sentence spliiting within the abstracts.\n",
    "\n",
    "We wish to retain that splitting instead of using spacy to split herein as spacy is imperfect due to all the acronyms in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "#     sents = spacy_split_sentences(f.read())\n",
    "# len(sents) # 179\n",
    "\n",
    "# sentences = []\n",
    "# with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "#     sents = f.read().splitlines() # 201\n",
    "#     for s in sents:\n",
    "#         sentences.append(spacy_split_sentences(s))\n",
    "# len(sentences) # still have 201, spacy found no new splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words and biwords from each of our sentences.\n",
    "Function assumes bag of words (lemmas) should have the actual words, and duplicate words are removed.\n",
    "\n",
    "Some approaches to using bag of words or lemmas assumes word appears greater than once in corpus. (which makes sense as meaningless to use if only occurs once across corpus?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### positives and negatives as bags of lemmas and lemma biwords\n",
    "Note, '{' to wrap array for psql, and \\N to leave empty field for deepdive reserved id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "201\n",
      "166\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "bags_of_lemmas = []\n",
    "for sentence in sentences:\n",
    "    lbw = to_lemmas_and_lemma_biwords(sentence)\n",
    "    bags_of_lemmas.append(lbw)\n",
    "print(len(bags_of_lemmas))\n",
    "\n",
    "print(len(neg_sentences))\n",
    "neg_bags_of_lemmas = []\n",
    "for sentence in neg_sentences:\n",
    "    lbw = to_lemmas_and_lemma_biwords(sentence)\n",
    "    neg_bags_of_lemmas.append(lbw)\n",
    "print(len(neg_bags_of_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save bags of lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ccarey/Documents/Projects/NAMI/rdoc/results\n"
     ]
    }
   ],
   "source": [
    "wkdir = '/Users/ccarey/Documents/Projects/NAMI/rdoc/results'\n",
    "%cd $wkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving positives and negatives\n",
    "- #TODO: the positives are not strictly positive yet, we still need to remove those annoatations from 'irrelevant' docs.\n",
    "- remember, positves had '{{' tags somewhere in sentence.\n",
    "- we are skipping the 'neutral' sentences, i.e. not saving anything from a positive document.\n",
    "- tmp_neg are any sentence from documents that were not positive.\n",
    "\n",
    "Note, '{' to wrap array for psql, and \\N to leave empty field for deepdive reserved id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open('./annotations_processed/AR_mk_pos_bags-of-lemmas', 'w', encoding='utf-8') as f:\n",
    "    for b in bags_of_lemmas:\n",
    "        b_arr = ', '.join(b)\n",
    "        f.write(u'{{{}}}\\t{}\\t{}\\n'.format(b_arr, '+arousal', '\\N'))\n",
    "\n",
    "with codecs.open('./annotations_processed/AR_mk_tmp_neg_bags-of-lemmas', 'w', encoding='utf-8') as f:\n",
    "    for b in neg_bags_of_lemmas:\n",
    "        b_arr = ', '.join(b)\n",
    "        f.write(u'{{{}}}\\t{}\\t{}\\n'.format(b_arr, '-arousal', '\\N'))\n",
    "#./annotations_processed/AR_mk_tmp_neg_bags-of-lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc (some other features from spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # can iterate over (ents) entities, (noun_chunks), sentences (sents)\n",
    "# print(sentences[17])\n",
    "# print('.....')\n",
    "# for i in range(17,18):\n",
    "#     print(sentences[i])\n",
    "#     print('------')\n",
    "#     doc = nlp(sentences[i], tag=True)\n",
    "#     print('Entities :')\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent)\n",
    "#     print('Noun chunks :')\n",
    "#     for nchunk in doc.noun_chunks:\n",
    "#         print(nchunk)\n",
    "#     for sent in doc.sents:\n",
    "#          print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
