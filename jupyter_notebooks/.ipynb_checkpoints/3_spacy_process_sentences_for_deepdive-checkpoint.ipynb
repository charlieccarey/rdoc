{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good spacy intro for getting tokens\n",
    "https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "\n",
    "## Load spacy.io\n",
    "https://spacy.io/#install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import codecs\n",
    "from spacy.en import English\n",
    "nlp = English(parser=True, tagger=True) # so we can sentence parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples: using spacy iterating over tokens to get words and lemmas.\n",
    "https://spacy.io/docs#token\n",
    "\n",
    "The tokens have several properties.\n",
    "\n",
    "Note the lemma handles conversion to lower case for us.\n",
    "\n",
    "- lemma: canonical or conceptual form of word.\n",
    "- orthographic: Commonsense commonsense or 'common-sense' are three different orthographic words. They are each single orthographic words. 'common sense' is two orthographic words. The meaning does not change though the way they are written does.\n",
    "\n",
    "(https://www.sussex.ac.uk/webteam/gateway/file.php?name=essay---what-is-a-word.pdf&site=1)\n",
    "\n",
    "I'm guessing the ints for the orthographies and lemmas come from the English dictionary (or the doc? below) loaded for parsing.\n",
    "\n",
    "Note how probability differs by the orthographic representation. That it would do so might not be obvious from the documentation. The probability is log probability from a large corpus. Smaller values are rarer.\n",
    "\n",
    "https://spacy.io/docs#token-distributional\n",
    "\n",
    "TODO: test this by looking up a few words directly by their ints or vice versa. Maybe see strings and note when it is loaded.\n",
    "\n",
    "https://spacy.io/docs#stringstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: add back code that created sentences array, probably what we currently have in 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sentences[0])\n",
    "print('\\n')\n",
    "for i in range(0, len(sentences)):\n",
    "    parsed_data = nlp(sentences[i])\n",
    "    for i, token in enumerate(parsed_data):\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(token.orth, token.orth_, token.lemma, token.lemma_))\n",
    "        if i > 20:\n",
    "            break\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90778\tInterval\t16220\tinterval\n",
      "14156\tintervals\t16220\tinterval\n",
      "506\t(\t506\t(\n",
      "1020\t20\t1020\t20\n",
      "4674\tsymptoms\t11231\tsymptom\n",
      "489\twith\t489\twith\n",
      "469\ta\t469\ta\n",
      "1357\thealth\t1357\thealth\n",
      "498\t-\t498\t-\n",
      "4899\tcaring\t827\tcare\n",
      "2231\tprofessional\t2231\tprofessional\n",
      "487\t)\t487\t)\n",
      "506\t(\t506\t(\n",
      "6159\tP\t10149\tp\n",
      "1216743\t>\t1216743\t>\n",
      "130535\t0.003\t130535\t0.003\n",
      "487\t)\t487\t)\n",
      "419\t.\t419\t.\n",
      "\n",
      "\n",
      "Interval\t16220\tinterval\t-15.9418096542\n",
      "intervals\t16220\tinterval\t-12.7912664413\n",
      "symptoms\t11231\tsymptom\t-11.159992218\n",
      "with\t489\twith\t-5.24324989319\n",
      "health\t1357\thealth\t-9.29672241211\n",
      "caring\t827\tcare\t-11.2292251587\n",
      "professional\t2231\tprofessional\t-10.0627040863\n",
      "0.003\t130535\t0.003\t-16.5937042236\n"
     ]
    }
   ],
   "source": [
    "test = u'Interval intervals (20 symptoms with a health-caring professional) (P > 0.003).'\n",
    "\n",
    "parsed_data = nlp(test)\n",
    "\n",
    "for token in parsed_data:\n",
    "    print('{}\\t{}\\t{}\\t{}'.format(token.orth, token.orth_, token.lemma, token.lemma_))\n",
    "print('\\n')\n",
    "\n",
    "for token in parsed_data:\n",
    "    if len(token.lemma_) > 2:\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(token.orth_, token.lemma, token.lemma_, token.prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather positive and negative sentences for Arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the annotations for Arousal topic as annotated by Mark.\n",
    "\n",
    "    See 2_spacy_parse_annotations notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wkdir = 'rdoc/results'\n",
    "%cd $wkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     201    5280   38255 ./annotations_processed/AR_mk_pos\n",
      "     211    4215   29329 ./annotations_processed/AR_mk_not_pos\n",
      "annotations_processed/AR_mk_pos: text/plain; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "!wc ./annotations_processed/AR_mk_pos\n",
    "!wc ./annotations_processed/AR_mk_not_pos\n",
    "! file -I annotations_processed/AR_mk_pos # utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "    sents = f.read().splitlines() # 201\n",
    "    for s in sents:\n",
    "        s = s.replace('{{', '').replace('}}', '').strip()\n",
    "        sentences.append(s)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a temporary set of negative sentences for Arousal by Mark.\n",
    "\n",
    "    See 2_spacy_parse_annotations notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sentences = []\n",
    "with codecs.open('./annotations_processed/AR_mk_tmp_neg', mode='r', encoding='utf-8') as f:\n",
    "    neg_sents = f.read().splitlines() # 201\n",
    "    for s in neg_sents:\n",
    "        s = s.replace('{{', '').replace('}}', '').strip()\n",
    "        neg_sentences.append(s)\n",
    "len(neg_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spacy to create bags of words from each sentence.\n",
    "TODO: Research which situations might suggesting unique rather than repeats of the words might be useful. Think this has to do in part with size of the text? Or is simply something to explore in any data mining task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Start postgres if desire to test array wrapping in postgres\n",
    "#!pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spacy helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_split_sentences(text):\n",
    "    sentences = []\n",
    "    #doc = nlp(text.decode('utf8')) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    doc = nlp(text) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    for span in doc.sents:\n",
    "        #sentences.append(u''.join(doc[i].string for i in range(span.start, span.end)).encode('utf-8').strip())\n",
    "        sentences.append(''.join(doc[i].string for i in range(span.start, span.end)))#.strip())\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_lemma_gt_len(text, length=2):\n",
    "    '''Create bag of unique lemmas, requiring lemma length > length\n",
    "    \n",
    "    Note: setting length to 1 may mess up our postgres arrays as we would\n",
    "    get commas here, unless we were to quote everything.\n",
    "    '''\n",
    "    tokens = []\n",
    "    #doc = nlp(text.decode('utf8')) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    parsed_data = nlp(text) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    for token in parsed_data:\n",
    "        if len(token.lemma_) > length:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    return(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'rotten', u'there', u'inside', u'apple']\n",
      "worm inside, rotten worm, tell me, once tell, inside ., apple once, me there, there be, a rotten\n"
     ]
    }
   ],
   "source": [
    "def spacy_lemma_biwords_gt_len(text, length=3):\n",
    "    '''Create bag of unique bi-lemmas, requiring lemma length > length\n",
    "    \n",
    "    We are crudely eliminating any bi-lemmas that have commas in them to save us in loading postgres arrays.\n",
    "    '''\n",
    "    biwords = []\n",
    "    parsed_data = nlp(text)\n",
    "    skip_chars = [',', '\"', \"'\"]\n",
    "    for i in range(1, len(parsed_data) - 1):\n",
    "        skip = False\n",
    "        biword = u'{} {}'.format(parsed_data[i].lemma_.lower(), parsed_data[i+1].lemma_.lower())\n",
    "        if (parsed_data[i].lemma_ in skip_chars or parsed_data[i+1].lemma_ in skip_chars):\n",
    "            skip = True\n",
    "        if len(biword) > length and not skip:\n",
    "            biwords.append(biword)\n",
    "    return(list(set(biwords)))\n",
    "\n",
    "test = 'A good, apple once told me there was a rotten worm inside.'.decode('utf8')\n",
    "res = spacy_lemma_gt_len(test, length=4)\n",
    "print(res)\n",
    "res = spacy_lemma_biwords_gt_len(test, length=4)\n",
    "# ', '.join(res) # note, flattens formatting.\n",
    "print(', '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_lemmas_and_lemma_biwords(sentence, lemma_len=2, bi_lemma_len=4):\n",
    "    lemma = spacy_lemma_gt_len(sentence, length=lemma_len)\n",
    "    lemma_biwords = spacy_lemma_biwords_gt_len(sentence, length=bi_lemma_len)\n",
    "    return(lemma + lemma_biwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidenote: what do we use as sentences?\n",
    "\n",
    "We preserve sentences as parsed in stop 2_spacy_parse_annotations notebook.\n",
    "\n",
    "Note, we did some manual sentence spliiting within the abstracts.\n",
    "\n",
    "We wish to retain that splitting instead of using spacy to split herein as spacy is imperfect due to all the acronyms in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "#     sents = spacy_split_sentences(f.read())\n",
    "# len(sents) # 179\n",
    "\n",
    "# sentences = []\n",
    "# with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "#     sents = f.read().splitlines() # 201\n",
    "#     for s in sents:\n",
    "#         sentences.append(spacy_split_sentences(s))\n",
    "# len(sentences) # still have 201, spacy found no new splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words and biwords from each of our sentences.\n",
    "Function assumes bag of words (lemmas) should have the actual words, and duplicate words are removed.\n",
    "\n",
    "Some approaches to using bag of words or lemmas assumes word appears greater than once in corpus. (which makes sense as meaningless to use if only occurs once across corpus?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex. large lemmas from 1st sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "Sociodemographic inequalities in the stage of diagnosis and cancer survival may be partly due to differences in the appraisal interval (time from noticing a bodily change to perceiving a reason to discuss symptoms with a health-care professional).\n",
      "professional\n",
      "sociodemographic\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(sentence)\n",
    "    lemma_6 = spacy_lemma_gt_len(sentence, length=10)\n",
    "    print('\\n'.join(lemma_6))\n",
    "    if i >= 0:\n",
    "        break\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Test unicode OK across all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## positives\n",
    "# print(len(sentences))\n",
    "# bags_of_lemmas = []\n",
    "# for sentence in sentences:\n",
    "#     lemma_2 = spacy_lemma_gt_len(sentence, n=2)\n",
    "#     bags_of_lemmas.append(lemma_2)\n",
    "# print(len(bags_of_lemmas))\n",
    "\n",
    "## negatives\n",
    "# print(len(neg_sentences))\n",
    "# neg_bags_of_lemmas = []\n",
    "# for sentence in neg_sentences:\n",
    "#     lemma_2 = spacy_lemma_gt_len(sentence, n=2)\n",
    "#     neg_bags_of_lemmas.append(lemma_2)\n",
    "# print(len(neg_bags_of_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### positives and negatives as bags of lemmas and lemma biwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "201\n",
      "166\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "bags_of_lemmas = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lbw = to_lemmas_and_lemma_biwords(sentence)\n",
    "    bags_of_lemmas.append(lbw)\n",
    "print(len(bags_of_lemmas))\n",
    "\n",
    "print(len(neg_sentences))\n",
    "neg_bags_of_lemmas = []\n",
    "for sentence in neg_sentences:\n",
    "    lbw = to_lemmas_and_lemma_biwords(sentence)\n",
    "    neg_bags_of_lemmas.append(lbw)\n",
    "print(len(neg_bags_of_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wkdir = './rdoc/results'\n",
    "%cd $wkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving positives and negatives\n",
    "- #TODO: the positives are not strictly positive yet, we still need to remove those annoatations from 'irrelevant' docs.\n",
    "- remember, positves had '{{' tags somewhere in sentence.\n",
    "- we are skipping the 'neutral' sentences, i.e. not saving anything from a positive document.\n",
    "- tmp_neg are any sentence from documents that were not positive.\n",
    "\n",
    "Note, '{' to wrap array for psql, and \\N to leave empty field for deepdive reserved id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open('./annotations_processed/AR_mk_pos_bags-of-lemmas', 'w', encoding='utf-8') as f:\n",
    "    for b in bags_of_lemmas:\n",
    "        b_arr = ', '.join(b)\n",
    "        f.write(u'{{{}}}\\t{}\\t{}\\n'.format(b_arr, '+arousal', '\\N'))\n",
    "\n",
    "with codecs.open('./annotations_processed/AR_mk_tmp_neg_bags-of-lemmas', 'w', encoding='utf-8') as f:\n",
    "    for b in neg_bags_of_lemmas:\n",
    "        b_arr = ', '.join(b)\n",
    "        f.write(u'{{{}}}\\t{}\\t{}\\n'.format(b_arr, '-arousal', '\\N'))\n",
    "#./annotations_processed/AR_mk_tmp_neg_bags-of-lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc (some other features from spacy)\n",
    "Can iterate over (ents) entities, (noun_chunks), sentences (sents)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(sentences[17])\n",
    "# print('.....')\n",
    "# for i in range(17,18):\n",
    "#     print(sentences[i])\n",
    "#     print('------')\n",
    "#     doc = nlp(sentences[i], tag=True)\n",
    "#     print('Entities :')\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent)\n",
    "#     print('Noun chunks :')\n",
    "#     for nchunk in doc.noun_chunks:\n",
    "#         print(nchunk)\n",
    "#     for sent in doc.sents:\n",
    "#          print(sent)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
