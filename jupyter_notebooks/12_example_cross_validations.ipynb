{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate a way to do cross validation with DeepDive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this is not K-fold cross validation. \n",
    "\n",
    "Cross validation here is essentially resampling with replacement.\n",
    "\n",
    "We simply create the app once, then initdb and run and accumulate stats for each run.\n",
    "\n",
    "We'll recreate one of our previous apps here, and loop through multiple initdb and run in order to collect our stats.\n",
    "\n",
    "## If want true k-fold cross-validation:\n",
    "It would be a little more difficult to do K-fold cross validation because DeepDive creates the test and training splits itself randomly.\n",
    "\n",
    "And note, there is randomness both in the set selection for training and test AND in the specific number of elements in each of these sets. \n",
    "\n",
    "For example, when we specify in the apps .conf file 75% training, 25% test, DeepDive might instead take 73% and 27%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pg_ctl: another server might be running; trying to start server anyway\r\n",
      "server starting\r\n"
     ]
    }
   ],
   "source": [
    "!pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start # deepdive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the same app creation method and one of the experiment sets as previously.\n",
    "The app creation is same as in ./11_2_aud_per_vs_others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import errno\n",
    "import glob\n",
    "\n",
    "dd_sent_sources = '/Users/ccarey/Documents/Projects/NAMI/rdoc/tasks/task_data_sentences'\n",
    "dd_app_dir = '/Users/ccarey/Documents/Projects/NAMI/rdoc/tasks/deepdive_app'\n",
    "templates = '/Users/ccarey/Documents/Projects/NAMI/rdoc/tasks/templates_deepdive_app_bagofwords/'\n",
    "conf_matrix_r_src = '/Users/ccarey/Documents/Projects/NAMI/rdoc/scripts/report_dd_confusion_matrix.R' # creates tsv and pdf reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dd_app(template_dir, app_name, input_raw, input_annotated):\n",
    "    '''Sets up a DeepDive app based and populates the input data'''\n",
    "    try:\n",
    "        shutil.copytree(template_dir, app_name)\n",
    "    except OSError as err:\n",
    "        print(\"Error copying {} to {}: {}\".format(template_dir, app_name, err))\n",
    "    # create / overwrite unique postgres db name for this app:\n",
    "    with open(os.path.join(app_name, 'db.url'), 'w') as f:\n",
    "        f.write('postgresql://localhost/{}\\n'.format(app_name))\n",
    "    try:\n",
    "        shutil.copyfile(input_raw, os.path.join(app_name, 'input', 'raw_sentences'))\n",
    "    except OSError as err:\n",
    "        print(\"Error copying raw sentences: {}\".format(err))\n",
    "    try:\n",
    "        shutil.copyfile(input_annotated, os.path.join(app_name, 'input', 'annotated_sentences'))\n",
    "    except OSError as err:\n",
    "        print(\"Error copying annotated sentences: {}\".format(err))\n",
    "\n",
    "def my_dd_table_sql_str():\n",
    "    '''Used to populate cc_all_predictions.tsv or such table in \n",
    "    deepdive app.\n",
    "    \n",
    "    That tsv file in turn is used by our confusion matrix R Script to\n",
    "    generate stats and plots from our deepdive apps.\n",
    "    '''\n",
    "    cmd = ('SELECT a.has_term, r.terms, a.sentence_id, expectation '\n",
    "           'FROM '\n",
    "           '_annotated_sentences_has_term_inference as a JOIN '\n",
    "           '_raw_sentences as r ON '\n",
    "           'a.sentence_id = r.sentence_id '\n",
    "           'ORDER BY a.sentence_id')\n",
    "    return(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a deepdive app named according to its input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ccarey/Documents/Projects/NAMI/rdoc/tasks/deepdive_app\n"
     ]
    }
   ],
   "source": [
    "# all apps must be at depth = 1, in a deepdive_app home directory, in our case 'deepdive_app/'\n",
    "%cd {dd_app_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we know our input data before trying to create the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AP_2_1000_vs_AR_1_1000_pdx_AP_1_1000_cval'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_collections = !ls -d {dd_sent_sources}/Auditory_2_1000__vs__Arousal_1_1000__pdx__un_Aud_1_1000\n",
    "sentence_input = sentence_collections[0]\n",
    "app_name = 'AP_2_1000_vs_AR_1_1000_pdx_AP_1_1000_cval'\n",
    "app_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating deepdive_app AP_2_1000_vs_AR_1_1000_pdx_AP_1_1000_cval at : /Users/ccarey/Documents/Projects/NAMI/rdoc/tasks/deepdive_app\n"
     ]
    }
   ],
   "source": [
    "print('Creating deepdive_app {} at : {}'.format(app_name, os.getcwd()))\n",
    "create_dd_app(template_dir=templates, \n",
    "              app_name = app_name,\n",
    "              input_raw=os.path.join(sentence_input, 'raw_sentences'),\n",
    "              input_annotated=os.path.join(sentence_input, 'annotated_sentences'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop through app 10 times, collecting stats.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ccarey/Documents/Projects/NAMI/rdoc/tasks/deepdive_app\n",
      "/Users/ccarey/Documents/Projects/NAMI/rdoc/tasks/deepdive_app/AP_2_1000_vs_AR_1_1000_pdx_AP_1_1000_cval\n"
     ]
    }
   ],
   "source": [
    "cmd = 'SELECT a.has_term, r.terms, a.sentence_id, expectation FROM _annotated_sentences_has_term_inference as a JOIN _raw_sentences as r ON a.sentence_id = r.sentence_id ORDER BY a.sentence_id'\n",
    "\n",
    "%cd {dd_app_dir}\n",
    "%cd {app_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TODO: we could make this more pythonic...\n",
    "\n",
    "# In each loop, the RScript is generating new stats and confusion matrix files, \n",
    "# we accumulate just the test portion each time.\n",
    "# The '' as last argument to R was an alternate title for plots? \n",
    "def run_deepdive(run_stats_fname):\n",
    "    !deepdive initdb 1> /dev/null 2>&1\n",
    "    !deepdive run 1> /dev/null 2>&1\n",
    "    !deepdive sql eval '{cmd}' format=tsv > cc_all_predictions.tsv # yes, \"{cmd}\" must be single or double quoted\n",
    "    !RScript {conf_matrix_r_src} cc_all_predictions.tsv \"{run_stats_fname}\" '' 1> /dev/null 2>&1\n",
    "\n",
    "for i in range(0,10):\n",
    "    run_stats_fname = app_name + str(i)\n",
    "    run_deepdive(run_stats_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = '*cval*' + 'test_only*' + 'confmatr.tsv'\n",
    "!cat {pattern} > 'cv_conf_matrix.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = '*cval*' + 'test_only*' + 'stats.tsv'\n",
    "!cat {pattern} >> 'cv_stats.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!grep '\"Specificity\\|\"Accuracy\\|Sensitivity\"' cv_stats.tsv | sort -s -d -k 1,1 > cc_cross_validation.tsv\n",
    "\n",
    "#cat cv_stats_matrix.tsv  | grep '\"Specificity\\|\"Accuracy\\|Sensitivity\"' | sort -s -d -k 1,1 > cc_cross_validation.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the plot of the cross validation statistics.\n",
    "Run the following code in R in this deepdive app."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: make this into one of our scripts.\n",
    "# R\n",
    "library(ggplot2)\n",
    "t <- read.table('cc_cross_validation.tsv', col.names = c('stat', 'value'), sep='\\t')\n",
    "\n",
    "pdf('cross_validation_plot.pdf')\n",
    "ggplot(data=t, aes( x=as.factor(stat), y=value, color=stat)) + geom_point() + ggtitle('Cross validation plot larger Auditory set vs Arousal') + theme(axis.text.x = element_text(angle = 90)) + xlab('Cross validation Statistic')\n",
    "dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
