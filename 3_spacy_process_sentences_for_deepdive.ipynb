{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Start postgres if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good spacy intro for getting tokens\n",
    "https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "\n",
    "## Load spacy.io\n",
    "https://spacy.io/#install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import codecs\n",
    "from spacy.en import English\n",
    "nlp = English(parser=True, tagger=True) # so we can sentence parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples: using spacy iterating over tokens to get words and lemmas.\n",
    "\n",
    "The tokens have many properties we can aceess.\n",
    "\n",
    "Note the lemma handles conversion to lower case for us.\n",
    "\n",
    "#TODO: what does Orth mean? Obviously it is the word or punctuation in its original capitalization, but is their a technical definition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a64194645f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparsed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print('\\n')\n",
    "for i in range(0, len(sentences)):\n",
    "    parsed_data = nlp(sentences[i])\n",
    "    for i, token in enumerate(parsed_data):\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(token.orth, token.orth_, token.lemma, token.lemma_))\n",
    "        if i > 20:\n",
    "            break\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90778\tInterval\t16220\tinterval\n",
      "14156\tintervals\t16220\tinterval\n",
      "506\t(\t506\t(\n",
      "1020\t20\t1020\t20\n",
      "4674\tsymptoms\t11231\tsymptom\n",
      "489\twith\t489\twith\n",
      "469\ta\t469\ta\n",
      "1357\thealth\t1357\thealth\n",
      "498\t-\t498\t-\n",
      "4899\tcaring\t827\tcare\n",
      "2231\tprofessional\t2231\tprofessional\n",
      "487\t)\t487\t)\n",
      "506\t(\t506\t(\n",
      "6159\tP\t10149\tp\n",
      "1216743\t>\t1216743\t>\n",
      "130535\t0.003\t130535\t0.003\n",
      "487\t)\t487\t)\n",
      "419\t.\t419\t.\n",
      "\n",
      "\n",
      "Interval\t16220\tinterval\t-15.9418096542\n",
      "intervals\t16220\tinterval\t-12.7912664413\n",
      "symptoms\t11231\tsymptom\t-11.159992218\n",
      "with\t489\twith\t-5.24324989319\n",
      "health\t1357\thealth\t-9.29672241211\n",
      "caring\t827\tcare\t-11.2292251587\n",
      "professional\t2231\tprofessional\t-10.0627040863\n",
      "0.003\t130535\t0.003\t-16.5937042236\n"
     ]
    }
   ],
   "source": [
    "test = u'Interval intervals (20 symptoms with a health-caring professional) (P > 0.003).'\n",
    "\n",
    "parsed_data = nlp(test)\n",
    "\n",
    "for token in parsed_data:\n",
    "    print('{}\\t{}\\t{}\\t{}'.format(token.orth, token.orth_, token.lemma, token.lemma_))\n",
    "print('\\n')\n",
    "\n",
    "for token in parsed_data:\n",
    "    if len(token.lemma_) > 2:\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(token.orth_, token.lemma, token.lemma_, token.prob)) # probability is from large corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather positive and negative sentences for Arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the annotations for Arousal topic as annotated by Mark.\n",
    "\n",
    "    See 2_spacy_parse_annotations notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ccarey/Documents/Projects/NAMI/rdoc/results\n"
     ]
    }
   ],
   "source": [
    "wkdir = '/Users/ccarey/Documents/Projects/NAMI/rdoc/results'\n",
    "%cd $wkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     201    5280   38255 ./annotations_processed/AR_mk_pos\n",
      "     211    4215   29329 ./annotations_processed/AR_mk_not_pos\n",
      "annotations_processed/AR_mk_pos: text/plain; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "!wc ./annotations_processed/AR_mk_pos\n",
    "!wc ./annotations_processed/AR_mk_not_pos\n",
    "! file -I annotations_processed/AR_mk_pos # utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "    sents = f.read().splitlines() # 201\n",
    "    for s in sents:\n",
    "        s = s.replace('{{', '').replace('}}', '').strip()\n",
    "        sentences.append(s)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a temporary set of negative sentences for Arousal by Mark.\n",
    "\n",
    "    See 2_spacy_parse_annotations notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sentences = []\n",
    "with codecs.open('./annotations_processed/AR_mk_tmp_neg', mode='r', encoding='utf-8') as f:\n",
    "    neg_sents = f.read().splitlines() # 201\n",
    "    for s in neg_sents:\n",
    "        s = s.replace('{{', '').replace('}}', '').strip()\n",
    "        neg_sentences.append(s)\n",
    "len(neg_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spacy to create bags of words from each sentence.\n",
    "#TODO: not sure if we should unique the bag of words. If encoding to integer feature vector, we would be counting occurrences.\n",
    "### spacy helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_split_sentences(text):\n",
    "    sentences = []\n",
    "    #doc = nlp(text.decode('utf8')) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    doc = nlp(text) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    for span in doc.sents:\n",
    "        #sentences.append(u''.join(doc[i].string for i in range(span.start, span.end)).encode('utf-8').strip())\n",
    "        sentences.append(''.join(doc[i].string for i in range(span.start, span.end)))#.strip())\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_lemma_gt_len(text, length=2):\n",
    "    '''Create bag of unique lemmas, requiring lemma length > length\n",
    "    \n",
    "    Note: setting length to 1 may mess up our postgres arrays as we would\n",
    "    get commas here, unless we were to quote everything.\n",
    "    '''\n",
    "    tokens = []\n",
    "    #doc = nlp(text.decode('utf8')) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    parsed_data = nlp(text) #\"This is a sentence. Here's another...\".decode('utf8'))\n",
    "    for token in parsed_data:\n",
    "        if len(token.lemma_) > length:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    return(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'rotten', u'there', u'inside', u'apple']\n",
      "worm inside, rotten worm, tell me, once tell, inside ., apple once, me there, there be, a rotten\n"
     ]
    }
   ],
   "source": [
    "def spacy_lemma_biwords_gt_len(text, length=3):\n",
    "    '''Create bag of unique bi-lemmas, requiring lemma length > length\n",
    "    \n",
    "    We are crudely eliminating any bi-lemmas that have commas in them to save us in loading postgres arrays.\n",
    "    '''\n",
    "    biwords = []\n",
    "    parsed_data = nlp(text)\n",
    "    skip_chars = [',', '\"', \"'\"]\n",
    "    for i in range(1, len(parsed_data) - 1):\n",
    "        skip = False\n",
    "        biword = u'{} {}'.format(parsed_data[i].lemma_.lower(), parsed_data[i+1].lemma_.lower())\n",
    "        if (parsed_data[i].lemma_ in skip_chars or parsed_data[i+1].lemma_ in skip_chars):\n",
    "            skip = True\n",
    "        if len(biword) > length and not skip:\n",
    "            biwords.append(biword)\n",
    "    return(list(set(biwords)))\n",
    "\n",
    "test = 'A good, apple once told me there was a rotten worm inside.'.decode('utf8')\n",
    "res = spacy_lemma_gt_len(test, length=4)\n",
    "print(res)\n",
    "res = spacy_lemma_biwords_gt_len(test, length=4)\n",
    "# ', '.join(res) # note, flattens formatting.\n",
    "print(', '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_lemmas_and_lemma_biwords(sentence, lemma_len=2, bi_lemma_len=4):\n",
    "    lemma = spacy_lemma_gt_len(sentence, length=lemma_len)\n",
    "    lemma_biwords = spacy_lemma_biwords_gt_len(sentence, length=bi_lemma_len)\n",
    "    return(lemma + lemma_biwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidenote: what do we use as sentences?\n",
    "\n",
    "We preserve sentences as parsed in stop 2_spacy_parse_annotations notebook.\n",
    "\n",
    "Note, we did some manual sentence spliiting within the abstracts.\n",
    "\n",
    "We wish to retain that splitting instead of using spacy to split herein as spacy is imperfect due to all the acronyms in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "#     sents = spacy_split_sentences(f.read())\n",
    "# len(sents) # 179\n",
    "\n",
    "# sentences = []\n",
    "# with codecs.open('./annotations_processed/AR_mk_pos', mode='r', encoding='utf-8') as f:\n",
    "#     sents = f.read().splitlines() # 201\n",
    "#     for s in sents:\n",
    "#         sentences.append(spacy_split_sentences(s))\n",
    "# len(sentences) # still have 201, spacy found no new splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words and biwords from each of our sentences.\n",
    "Function assumes bag of words (lemmas) should have the actual words, and duplicate words are removed.\n",
    "\n",
    "Some approaches to using bag of words or lemmas assumes word appears greater than once in corpus. (which makes sense as meaningless to use if only occurs once across corpus?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex. large lemmas from 1st sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "Sociodemographic inequalities in the stage of diagnosis and cancer survival may be partly due to differences in the appraisal interval (time from noticing a bodily change to perceiving a reason to discuss symptoms with a health-care professional).\n",
      "professional\n",
      "sociodemographic\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(sentence)\n",
    "    lemma_6 = spacy_lemma_gt_len(sentence, length=10)\n",
    "    print('\\n'.join(lemma_6))\n",
    "    if i >= 0:\n",
    "        break\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Test unicode OK across all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## positives\n",
    "# print(len(sentences))\n",
    "# bags_of_lemmas = []\n",
    "# for sentence in sentences:\n",
    "#     lemma_2 = spacy_lemma_gt_len(sentence, n=2)\n",
    "#     bags_of_lemmas.append(lemma_2)\n",
    "# print(len(bags_of_lemmas))\n",
    "\n",
    "## negatives\n",
    "# print(len(neg_sentences))\n",
    "# neg_bags_of_lemmas = []\n",
    "# for sentence in neg_sentences:\n",
    "#     lemma_2 = spacy_lemma_gt_len(sentence, n=2)\n",
    "#     neg_bags_of_lemmas.append(lemma_2)\n",
    "# print(len(neg_bags_of_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### positives and negatives as bags of lemmas and lemma biwords\n",
    "Note, '{' to wrap array for psql, and \\N to leave empty field for deepdive reserved id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "201\n",
      "166\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "bags_of_lemmas = []\n",
    "for sentence in sentences:\n",
    "    lbw = to_lemmas_and_lemma_biwords(sentence)\n",
    "    bags_of_lemmas.append(lbw)\n",
    "print(len(bags_of_lemmas))\n",
    "\n",
    "print(len(neg_sentences))\n",
    "neg_bags_of_lemmas = []\n",
    "for sentence in neg_sentences:\n",
    "    lbw = to_lemmas_and_lemma_biwords(sentence)\n",
    "    neg_bags_of_lemmas.append(lbw)\n",
    "print(len(neg_bags_of_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save bags of lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ccarey/Documents/Projects/NAMI/rdoc/results\n"
     ]
    }
   ],
   "source": [
    "wkdir = '/Users/ccarey/Documents/Projects/NAMI/rdoc/results'\n",
    "%cd $wkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving positives and negatives\n",
    "- #TODO: the positives are not strictly positive yet, we still need to remove those annoatations from 'irrelevant' docs.\n",
    "- remember, positves had '{{' tags somewhere in sentence.\n",
    "- we are skipping the 'neutral' sentences, i.e. not saving anything from a positive document.\n",
    "- tmp_neg are any sentence from documents that were not positive.\n",
    "\n",
    "Note, '{' to wrap array for psql, and \\N to leave empty field for deepdive reserved id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open('./annotations_processed/AR_mk_pos_bags-of-lemmas', 'w', encoding='utf-8') as f:\n",
    "    for b in bags_of_lemmas:\n",
    "        b_arr = ', '.join(b)\n",
    "        f.write(u'{{{}}}\\t{}\\t{}\\n'.format(b_arr, '+arousal', '\\N'))\n",
    "\n",
    "with codecs.open('./annotations_processed/AR_mk_tmp_neg_bags-of-lemmas', 'w', encoding='utf-8') as f:\n",
    "    for b in neg_bags_of_lemmas:\n",
    "        b_arr = ', '.join(b)\n",
    "        f.write(u'{{{}}}\\t{}\\t{}\\n'.format(b_arr, '-arousal', '\\N'))\n",
    "#./annotations_processed/AR_mk_tmp_neg_bags-of-lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc (some other features from spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # can iterate over (ents) entities, (noun_chunks), sentences (sents)\n",
    "# print(sentences[17])\n",
    "# print('.....')\n",
    "# for i in range(17,18):\n",
    "#     print(sentences[i])\n",
    "#     print('------')\n",
    "#     doc = nlp(sentences[i], tag=True)\n",
    "#     print('Entities :')\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent)\n",
    "#     print('Noun chunks :')\n",
    "#     for nchunk in doc.noun_chunks:\n",
    "#         print(nchunk)\n",
    "#     for sent in doc.sents:\n",
    "#          print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
